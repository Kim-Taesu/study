#### 구조적 스트리밍의 기초

- 스트리밍 연산은 배치 연산과 동일하게 표현한다.
- 사용자가 스트림 처리용 코드와 목적지를 정의하면 구조적 스트리밍 엔진에서 신규 데이터에 대한 증분 및 연속형 쿼리를 실행
- 코드 생성, 쿼리 최적화 등의 기능을 지원하는 카탈리스트 엔진을 사용해 연산에 대한 논리적 명령을 처리
- 핵심 아이디어
  - 스트림 데이터를 데이터가 계속해서 추가되는 테이블처럼 다룬다.
  - 스트리밍 잡은 계속해서 신규 입력 데이터를 확인 및 처리
  - 필요한 경우 상태 저장소에 있는 일부 상태를 갱신해 결과를 변경

#### 입력 소스

- 아프카 카프카
- HDFS나 S3 등 분산 파일 시스템의 파일
- 테스트용 소켓 소스

#### 싱크

- 스트림의 결과를 저장할 목적지를 명시
- 싱크와 실행 엔진은 데이터 치리의 진행 상황을 신뢰도 있고 정확하게 추적하는 역할을 한다.
- 출력용 싱크 목록
  - 아파치 카프카 
  - 거의 모든 파일 포맷
  - 출력 레코드에 임의 연산을 실행하는 `foreach` 싱크

#### 출력 모드

- 발생할 수 있는 상황
  - 신규 정보만 추가하려는 경우
  - 바뀐 정보로 기존 로우를 갱신하려는 경우
  - 매번 전체 결과를 덮어쓰려는 경우
- 출력 모드 목록
  - append : 싱크에 신규 레코드만 추가
    - 마지막 트리거 이후 결과 테이블에 추가 된 새 행만 외부 스토리지에 기록
    - 결과 테이블의 기존 행이 변경되지 않는 쿼리에만 적용.
    - 워터 마크가 없는 스트리밍 DataFrame / DataSet에 스트리밍 집계가있는 경우 지원x
  - update : 변경 대상 레코드 자체를 갱신
    - 마지막 트리거 이후 결과 테이블에서 업데이트 된 행만 외부 스토리지에 기록
    - 마지막 트리거 이후 변경된 행만 출력한다는 점에서 `complete` 모드와 다르다.
    - 쿼리에 집계가 포함되어 있지 않으면 추가 모드와 같다.
  - complete : 전체 출력 내용 재작성
    - 업데이트 된 결과 테이블 전체가 외부 저장소에 기록
    - 전체 테이블 쓰기 처리 방법을 결정하는 것은 스토리지 커넥터에 달려 있습니다.

#### 트리거

- 데이터 출력 시점을 정의
- 구조적 스트리밍에서 언제 신규 데이터를 확인하고 결과를 갱신할지 정의
- 기본적으로 마지막 입력 데이터를 처리한 직후에 신규 입력 데이터를 조회해 최단 시간 내에 새로운 처리 결과를 만든다.
  - 파일 싱크를 사용하는 경우 작은 크기의 파일이 여러 개 생길 수 있다.
  - 따라서 처리 시간 기반의 트리거도 지원한다.

#### 이벤트 시간 처리

- 이벤트 시간 기준의 처리를 지원
- 무작위로 도착한 레코드 내부에 기록된 타임스탬프를 기준

---

#### 이벤트 시간 데이터 개념

- 이벤트 시간
- 워터 마크

#### 이벤트 시간

- 데이터에 기록된 시간 필드를 의미
- 데이터 생성 시간을 기준으로 처리
  - 데이터가 늦게 업로드 되거나 네트워크 지연으로 데이터의 순서과 뒤섞인 채 시스템으로 들어와도 처리할 수 있다.
- 입력 데이터를 테이블로 인식하므로 표준 SQL 연산자를 이용해 그룹화, 집계 그리고 윈도우 처리를 할 수 있다.
- 위 작업을 제어할 때 워터 마크를 사용

#### 워터 마크

- 시간 제한을 설정할 수 있는 스트리밍 시스템의 기능
- 늦게 들어온 이벤트를 어디까지 처리할지 시간을 제한할 수 있다.
- 이벤트 시간 처리를 지원하는 여러 시스템은 과거 데이터의 보관 주기를 제한하기 위해 워터마크를 사용
- 워터마크는 특정 이벤트 시간의 윈도우 결과를 출력하는 시점을 제어할 때도 사용

---

#### 데이터를 읽고 쓰는 장소 (소스와 싱크)

- 구조적 스트리밍은 몇 가지 실전용 소스와 싱크(파일과 아파치 카프카), 디버깅용 메모리 테이블 싱크 등을 지원

#### 파일 소스와 싱크

- 실전에서는 파케이, 텍스트, JSON 그리고 CSV 파일을 자주 사용
- 트리거 시 읽을 파일 수를 결정할 수 있다.
  - `maxFilesPerTrigger` 옵션으로 설정

#### 카프카 소스와 싱크

- 아파치 카프카
  - 데이터 스트림을 위한 발행-구독 방식의 분산형 시스템
  - 메시지 큐 방식 처럼 레코드의 스트림을 발행하고 구독하는 방식으로 사용
  - 발행된 메시지는 내고장성을 보장하는 저장소에 저장
  - (=분산형 버퍼)
- 레코드는 토픽으로 불리는 카테고리에 저장
  - 카프카의 각 레코드는 키, 값, 타입스탬프로 구성
  - 토픽은 바꿀 수 없는 레코드로 구성
  - 레코드의 위치를 오프셋이라 한다.
- 데이터를 읽는 동작 : subcribe
- 데이터를 쓰는 동작 : publish

#### foreach 싱크

- 각 파티션에서 임의의 연산을 병렬로 수행
- foreach 싱크를 사용하기 위해 ForeachWriter 인터페이스를 구현해야 한다.
  - open, process, close 세 가지 메소드를 가지고 있다.
  - 메소드는 트리거 후 출력을 생성할 때마다 호출

---

#### 데이터출력 방법 (출력 모드)

- append 모드
  - 새로운 로우가 결과 테이블에 추가되면 사용자가 명시한 트리거에 맞춰 싱크로 출력
  - 모든 로우를 한 번만 출력한다.
- complete 모드
  - 결과 테이블의 전체 상태를 싱크로 출력
  - 모든 데이터가 계속해서 변경될 수 있는 일부 상태 기반 데이터를 다룰 때 유용
- update 모드
  - 이전 출력 결과에서 변경된 로우만 싱크로 출력한다는 점을 제외하면 complete 모드와 유사
  - 이 모드를 지원하는 싱크는 반드시 저수준 업데이트를 지원해야 한다.

---

#### 데이터 출력 시점(트리거)

- 데이터를 싱크로 출력하는 시점을 제어하려면 트리거를 설정해야 한다.
- 너무 많은 수정이 발생한다면 데이터를 출력할 때 트리거를 사용
  - 싱크에 큰 부하가 발생하는 현상을 방지
  - 출력 파일의 크기를 제어
- 종류
  - 시간 기반의 주기형 트리거
  - 처리 단계를 수동으로 한 번만 실행할 수 있는 일회성 트리거

#### 처리 시간 기반 트리거

- 처리 주기를 문자열로 단순하게 지정

#### 일회성 트리거

- 스트리밍 잡을 일회성으로 실행하는 트리거 설정
- 운영 환경에서는 자주 실행되지 않는 잡을 수동으로 실행할 때 일회성 트리거를 사용