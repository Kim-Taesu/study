#### 스파크 API

- 저수준의 비 구조적 API
- 고수준의 구조적 API

#### DataFrame

- 대표적인 구조적 API

- 테이블의 데이터를 로우와 컬럼으로 표현

  - 컬럼과 컬럼의 타입을 정의한 목록을 스키마라고 한다.

- 수천 대의 컴퓨터에 분산되어 저장

- 대화형 모드가 아니라면 SparkSession 객체를 생성해야 한다.

  - ﻿﻿대화형 모드로 스파크를 시작하면 스파크 애플리케이션을 관리하는 SparkSession이 자동으로 생성

- 생성 예제

  - inferSchema 옵션으로 데이터를 조금 읽고 스키마 정보를 얻고 해당 로우의 데이터 타입을 스파크 데이터 타입에 맞게 분석한다.
  - header 옵션으로 첫 로우를 헤더로 지정

  ```scala
  val data = spark
  	.read
  	.option("inferSchema", "true")
  	.option("header", "true")
  	.csv("CSV_PATH.csv")
  ```

- DataFrame 객체에 explain 메서드를 호출하면 DataFrame의 계보나 스파크의 쿼리 실행 계획을 확인 할 수 있다.

  ```scala
  data.sort().explain()
  ```

  

#### Dataset

- 정적 데이터 타입에 맞는 코드, 정적 타입 코드를 지원하기 위해 고안된 스파크의 구조적 API

  - 정적 타입 코드 : 자료형이 고정된 언어 (자바, 스칼라, C, C++ 등)
  - 동적 타입 코드 : 파이썬, 자바스크립트

- 타입 안정성을 지원

- 다수의 소프트웨어 엔지니어가 잘 정의된 인터페이스로 상호작용하는 대규모 애플리케이션을 개발하는데 유용

- 타입 안정성 함수와 DataFrame을 사용해 비즈니스 로직을 신속하게 작성하는 예제

  ```scala
  import org.apache.spark.sql.SparkSession
  
  case class Flight(DEST_COUNTRY_NAME: String,
                    ORIGIN_COUNTRY_NAME: String,
                    count: BigInt)
  
  object Main {
    def main(args: Array[String]): Unit = {
  
      val spark = SparkSession.builder().master("local").appName("test").getOrCreate()
  
      import spark.implicits._
  
      val myrange = spark.range(1000).toDF("number")
  
  
      val flightsDF = spark.read
        .parquet("data/flight-data/parquet/2010-summary.parquet/")
      val flights = flightsDF.as[Flight]
    }
  }
  
  ```

  - collect, take 메서드를 호출하면 DataFrame을 구성하는 Row 타입의 객체가 아닌 Dataset에 매개변수로 지정한 타입의 객체를 반환
    - 타입 안정성 보장

#### 구조적 API

- Dataset
- DataFrame
- SQL 테이블과 뷰



#### 배치와 스트리밍 처리에서 구조적 API를 사용할 수 있다.



#### 중요한 기본 개념

- 타입형 / 비타입형 API의 개념과 차이점
- 핵심 용어
- 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식

#### 스파크 기본 실행

- 스파크는 트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델
- 사용자가 정의한 다수의 트랜스포메이션은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어낸다.
- 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행
- 트랜스포메이션과 액션으로 다루는 논리적 구조가 DataFrame과 Dataset이다.
- 새로운 DataFrame이나 Dataset을 만드려면 트랜스포메이션을 호출해야한다.

#### DataFrame과 Dataset

- 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션
- 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의한느 지연 연산의 실행 계획이다.
- 비타입형 : DataFrame
  - 스키마에 명시된 데이터 타입의 일치 여부를 런타임에서 확인
  - 스파크의 최적화된 내부 포맷을 사용할 수 있다.
- 타입형 : Dataset
  - 스키마에 명시된 데이터 타입의 일치 여부를 컨파일 타임에서 확인
  - 자바와 스칼라만 지원
    - 스칼라는 case class를 사용
    - 자바는 JavaBean 사용

---

#### DataSet

- 구조적 API의 기본 데이터 타입
- DtaFrame은 Row 타입의 Dataset이다.
- Dataset은 자바 가상 머신을 사용하는 언어인 스칼라와 자바에서만 사용 가능
- Dataset을 사용해 데이터셋의 각 로우를 구성하는 객체를 정의

#### Dataset을 사용할 시기

- DataFrame 기능만으로는 수행할 연산을 표할할 수 없는 경우
  - 복잡한 비즈니스 로직을 SQL이나 DataFrame 대신 단일 함수로 인코딩해야 하는 경우
- 성능 저하를 감수하더라도 타입 안전성을 가진 데이터 타입을 사용하고 싶은 경우

#### Dataset 생성

- 정의할 스키마를 미리 알고 있어야 한다.
- 스칼라 `case class` 구문을 사용해 데이터 타입을 정의



#### 스키마

- 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법
- DataFrame의 컬럼명과 데이터 타입을 정의



#### 스파크의 구조적 데이터 타입 개요

- 스파크는 사실상 프로그래밍 언어
- 스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 **카탈리스트** 엔진을 사용
  - 다양한 실행 최적화 기능 제공



#### 컬럼

- 정수형이나 문자열 같은 **단순 데이터 타입**, 배열이나 맵 같은 **복합 데이터 타입**, **null 값** 표현



#### 로우

- DataFrame의 레코드는 Row 타입으로 구성



#### 컬럼과 표현식

- 사용자는 표현식으로 DataFrame의 컬럼을 선택, 조작, 제거할 수 있다.
- 컬럼 내용을 수정하려면 반드시 DataFrame의 스파크 트랜스포메이션을 사용해야 한다.
- col 함수나 column 함수를 사용하여 컬럼을 생성하거나 참조
- 컬럼은 컬럼명을 카탈로그에 젖아된 정보와 비교하기 전까지 미확인 상태로 남는다.

---

#### 명시적 컬럼 참조

- col 메서드로 참조
  - 조인 시 유용하다.
- col 메서드를 사용해 명시적으로 컬럼을 정의하면 스파크는 분석기 실행 단계에서 컬럼 확인 절차를 생략한다.

---

#### 표현식

- DataFrame 레코드의 여러 값에 대한 트랜스포메이션 집합을 의미

- 여러 컬럼 명을 입력으로 받아 식별하고, '단일 값'을 만들기 위해 다양한 표현식을 각 레코드에 적용

  - 단일 값 : Map, Array 같은 복합 데이터 타입일수도 있다.

- expr 함수로 사용한다.

  - DataFrame의 컬럼 참조 가능 (밑의 두 식은 같다.)

    ```scala
    expr("somCol")
    col("somCol")
    ```

---

#### 표현식으로 컬럼 표현

- 컬럼은 표현식의 일부 기능을 제공

  - col() 함수를 호출해 컬럼에 트랜스포메이션을 수행하려면 반드시 컬럼 참조를 사용해야 한다.

- expr 함수의 인수로 표현식을 사용하면 표현식을 분석해 트랜스포메이션과 컬럼 참조를 알아낼 수 있다.

  - 다음 트랜스포메이션에 컬럼 참조를 전달 가능

- 아래 식은 모두 같은 트랜스 포메이션 과정을 거친다.

  - 스파크가 연산 순서를 지정하는 논리적 트리로 컴파일하기 때문

    ```scala
    expr("colTest - 5")
    col("colTest") - 5
    expr("colTest") - 5
    ```

- **컬럼은 단지 표현식이다.**

- **컬럼과 컬럼의 트랜스포메이션은 파싱된 표현식과 동일한 논리적 실행 계획으로 컴파일**

---

#### DataFrame 컬럼에 접근하기

- 프로그래밍 방식으로 컬럼에 접근할 때는 DataFrame의 columns 속성을 사용

#### 로우  생성하기

- 각 컬럼에 해당하는 값을 사용해 Row 객체를 직접 생성할 수 있다.
- Row 객체를 직접 생성하려면 DataFrame의 스키마와 같은 순서로 값을 명시해야 한다.

---



#### 구조적 API의 실행 과정

1. DataFrame / Dataset / SQL을 이용해 코드를 작성
2. 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환
3. 스파크는 물리적 실행 계획을 실행하기 위해 `Catalyst Optimizer` 에게 논리적 실행 계획을 전달한다.
   1. 이 과정에서 추가적인 최적화를 할 수 있는지 확인
4. 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행

- 스파크를 실행할 코드를 작성하고 console 또는 spark-submit으로 실행
- 카탈리스트 옵티마이저는 코드를 넘겨 받고 실제 실행 계획을 생성한다.
- 마지막으로 스파크는코드를 실행한 후 결과를 반환

![img](https://i0.wp.com/blog.knoldus.com/wp-content/uploads/2019/11/Fig-1-Plan.jpg?w=810&ssl=1)



#### 논리적 실행 계획

- 정의 : 수행되어야하는 모든 transformation의 추상적인 계획
  - 드라이버나 익스큐터의 정보를 고려하지 않는다.
  - SparkContext가 논리적 실행 계획을 저장하고 실행시킨다.
- 첫 번째 단계로 사용자 코드를 논리적 실행 계획으로 변환
  - 이 단계에서는 추상적 트랜스포메이션만 표현
  - 이 과정의 결과로 **검증 전 논리적 실행 계획**(Unresolved Logical Plan)이 반환된다.
  - 코드의 유효성과 테이블이나 컬럼의 존재 여부만을 판단
- 스파크 분석기(**Analyzer**)가 컬럼과 테이블을 검증
  - 카탈로그 : 데이터베이스 정보와 로우, 컬럼의 정보를 저장
  - 카탈로그, 모든 테이블의 저장소 그리고 DataFrame의 정보를 활용하여 검증
  - 필요한 테이블이나 컬럼이 카탈로그에 없다면 검증 전 논리적 실행 계획 분석을 멈추고 거절한다.
  - Analyzer의 결과로 **Resolved Logical Plan**이 생성된다.
- *Resolved Logical Plan*는 카탈리스트 옵티마이저(**Catalyst Optimizer**)로 전달된다.
  - Catalyst Optimizer:  조건절 푸시다운이나 선택절 구문을 이용해 논리적 실행 계획을 최적화하는 규칙의 모음
  - Catalyst Optimizer는 일반적으로 logical optimization을 수행한다.



#### 물리적 실행 계획

- 논리적 실행 계획을 **클러스터 환경에서 실행**하는 방법을 정의
- 최적화된 논리적 실행 계획을 토대로 물리적 실행 계획으로 생성하고 비용 모델을 이용해 최적의 전략을 선택한다.
  - 비용 비교 예 : 사용하려는 테이블의 크기나 파티션 수 등의 물리적 속성을 고려해 지정된 조인 연산 수행에 필요한 비용을 계산하고 비교
- 물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환된다.
- 스파크는 DataFrame, Dataset, SQL로 정의된 쿼리를 RDD 트랜스포메이션으로 컴파일한다.
- 물리적 실행 계획을 선정하고 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드를 실행

