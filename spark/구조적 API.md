#### 구조적 API

- Dataset
- DataFrame
- SQL 테이블과 뷰

#### 배치와 스트리밍 처리에서 구조적 API를 사용할 수 있다.

#### 중요한 기본 개념

- 타입형 / 비타입형 API의 개념과 차이점
- 핵심 용어
- 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식

#### 스파크 기본 실행

- 스파크는 트랜스포메이션의 처리 과정을 정의하는 분산 프로그래밍 모델
- 사용자가 정의한 다수의 트랜스포메이션은 지향성 비순환 그래프(DAG)로 표현되는 명령을 만들어낸다.
- 액션은 하나의 잡을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG 처리 프로세스를 실행
- 트랜스포메이션과 액션으로 다루는 논리적 구조가 DataFrame과 Dataset이다.
- 새로운 DataFrame이나 Dataset을 만드려면 트랜스포메이션을 호출해야한다.

#### DataFrame과 Dataset

- 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션
- 결과를 생성하기 위해 어떤 데이터에 어떤 연산을 적용해야 하는지 정의한느 지연 연산의 실행 계획이다.
- 비타입형 : DataFrame
  - 스키마에 명시된 데이터 타입의 일치 여부를 런타임에서 확인
  - 스파크의 최적화된 내부 포맷을 사용할 수 있다.
- 타입형 : Dataset
  - 스키마에 명시된 데이터 타입의 일치 여부를 컨파일 타임에서 확인
  - 자바와 스칼라만 지원
    - 스칼라는 case class를 사용
    - 자바는 JavaBean 사용

#### DataSet

- 구조적 API의 기본 데이터 타입
- DtaFrame은 Row 타입의 Dataset이다.
- Dataset은 자바 가상 머신을 사용하는 언어인 스칼라와 자바에서만 사용 가능
- Dataset을 사용해 데이터셋의 각 로우를 구성하는 객체를 정의

#### Dataset을 사용할 시기

- DataFrame 기능만으로는 수행할 연산을 표할할 수 없는 경우
  - 복잡한 비즈니스 로직을 SQL이나 DataFrame 대신 단일 함수로 인코딩해야 하는 경우
- 성능 저하를 감수하더라도 타입 안전성을 가진 데이터 타입을 사용하고 싶은 경우

#### Dataset 생성

- 정의할 스키마를 미리 알고 있어야 한다.
- 스칼라 `case class` 구문을 사용해 데이터 타입을 정의



#### 스키마

- 분산 컬렉션에 저장할 데이터 타입을 정의하는 방법
- DataFrame의 컬럼명과 데이터 타입을 정의

#### 스파크의 구조적 데이터 타입 개요

- 스파크는 사실상 프로그래밍 언어
- 스파크는 실행 계획 수립과 처리에 사용하는 자체 데이터 타입 정보를 가지고 있는 **카탈리스트** 엔진을 사용
  - 다양한 실행 최적화 기능 제공

#### 컬럼

- 정수형이나 문자열 같은 **단순 데이터 타입**, 배열이나 맵 같은 **복합 데이터 타입**, **null 값** 표현

#### 로우

- DataFrame의 레코드는 Row 타입으로 구성

#### 구조적 API의 실행 과정

1. DataFrame / Dataset / SQL을 이용해 코드를 작성
2. 정상적인 코드라면 스파크가 논리적 실행 계획으로 변환
3. 스파크는 논리적 실행 계획을 물리적 실행 계획으로 변환
   1. 이 과정에서 추가적인 최적화를 할 수 있는지 확인
4. 스파크는 클러스터에서 물리적 실행 계획(RDD 처리)을 실행

- 스파크를 실행할 코드를 작성하고 console 또는 spark-submit으로 실행
- 카탈리스트 옵티마이저는 코드를 넘겨 받고 실제 실행 계획을 생성한다.
- 마지막으로 스파크는코드를 실행한 후 결과를 반환

#### 논리적 실행 계획

- 첫 번째 단계로 사용자 코드를 논리적 실행 계획으로 변환
  - 이 단계에서는 추상적 트랜스포메이션만 표현
  - 드라이버나 익스큐터의 정보를 고려하지 않는다.
  - 이 과정의 결과로 **검증 전 논리적 실행 계획**이 반환된다.
  - 코드의 유효성과 테이블이나 컬럼의 존재 여부만을 판단
- 스파크 분석기가 컬럼과 테이블을 검증
  - 카탈로그 : 데이터베이스 정보와 로우, 컬럼의 정보를 저장
  - 카탈로그, 모든 테이블의 저장소 그리고 DataFrame의 정보를 활용하여 검증
  - 필요한 테이블이나 컬럼이 카탈로그에 없다면 검증 전 논리적 실행 계획이 만들어지지 않는다.
- 테이블과 컬럼에 대한 검증 결과는 카탈리스트 옵티마이저로 전달된다.
  - 카탈리스트 옵티마이저 :  조건절 푸시다운이나 선택절 구문을 이용해 논리적 실행 계획을 최적화하는 규칙의 모음

#### 물리적 실행 계획

- 논리적 실행 계획을 클러스터 환경에서 실행하는 방법을 정의
- 최적화된 논리적 실행 계획을 토대로 물리적 실행 계획으로 생성하고 비용 모델을 이용해 최적의 전략을 선택한다.
  - 비용 비교 예 : 사용하려는 테이블의 크기나 파티션 수 등의 물리적 속성을 고려해 지정된 조인 연산 수행에 필요한 비용을 계산하고 비교
- 물리적 실행 계획은 일련의 RDD와 트랜스포메이션으로 변환된다.
- 스파크는 DataFrame, Dataset, SQL로 정의된 쿼리를 RDD 트랜스포메이션으로 컴파일한다.
- 물리적 실행 계획을 선정하고 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드를 실행